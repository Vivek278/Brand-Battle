{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To6hPU2wAxoQ",
        "outputId": "9c583c75-1e8f-4205-f543-dba7cfbd32cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Aspects Found: ['product ki warrenty kamse kam mahine ho na jaruri hai mahineme iseme problem hu wa hai worst earphones', 'damaged product ek number hai matalab skullcandy ke inked se bhi supper paisa wasul original product thanks amazon ok best bluetooth earphone', 'ft received earbuds call ke time bat krne pr srrrr srrr ki awaj aa rhi h aaj', 'pocket good price good product superb bass effort nice thankful amazon superb bass effort nice thankful amazon fully paisa vasul still', 'others reviewers mentioned oxmmm yr gjb k base hai battery lfe mstsound quality oxmmm value']\n",
            "\n",
            "Final Analysis Results:\n",
            "                                                    average_sentiment   count\n",
            "product ki warrenty kamse kam mahine ho na jaru...           0.385804  1813.0\n",
            "damaged product ek number hai matalab skullcand...           0.449284  2215.0\n",
            "ft received earbuds call ke time bat krne pr sr...           0.199549   413.0\n",
            "pocket good price good product superb bass effo...           0.591251  3412.0\n",
            "others reviewers mentioned oxmmm yr gjb k base ...           0.855697  2446.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, sentiwordnet as swn, wordnet as wn\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def get_adjective_sentiment(word):\n",
        "    synsets = list(swn.senti_synsets(word))\n",
        "    if not synsets:\n",
        "        return 0.0\n",
        "\n",
        "    scores = []\n",
        "    for syn in synsets:\n",
        "        if syn.synset.pos() == 'a' or syn.synset.pos() == 's':\n",
        "            scores.append(syn.pos_score() - syn.neg_score())\n",
        "\n",
        "    if len(scores) > 0:\n",
        "        return sum(scores) / len(scores)\n",
        "    return 0.0\n",
        "\n",
        "def get_rake_keywords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    sentences = sent_tokenize(text)\n",
        "    phrase_list = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        phrase = []\n",
        "        for word in words:\n",
        "            if word.lower() not in stop_words and word.isalpha():\n",
        "                phrase.append(word.lower())\n",
        "            else:\n",
        "                if phrase:\n",
        "                    phrase_list.append(\" \".join(phrase))\n",
        "                    phrase = []\n",
        "        if phrase:\n",
        "            phrase_list.append(\" \".join(phrase))\n",
        "\n",
        "    word_freq = Counter()\n",
        "    word_degree = Counter()\n",
        "\n",
        "    for phrase in phrase_list:\n",
        "        words = phrase.split()\n",
        "        length = len(words)\n",
        "        for w in words:\n",
        "            word_freq[w] += 1\n",
        "            word_degree[w] += length\n",
        "\n",
        "    word_scores = {}\n",
        "    for w in word_freq:\n",
        "        word_scores[w] = word_degree[w] / word_freq[w]\n",
        "\n",
        "    phrase_scores = {}\n",
        "    for phrase in phrase_list:\n",
        "        if phrase not in phrase_scores:\n",
        "            score = 0\n",
        "            for w in phrase.split():\n",
        "                score += word_scores[w]\n",
        "            phrase_scores[phrase] = score\n",
        "\n",
        "    return sorted(phrase_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "def analyze_sentiment(text, aspect):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    aspect_words = aspect.split()\n",
        "    score = 0\n",
        "    count = 0\n",
        "    negations = {'not', 'no', 'never', 'none', 'neither', 'nor', 'hardly'}\n",
        "\n",
        "    for i, token in enumerate(tokens):\n",
        "        if token in aspect_words:\n",
        "            window_start = max(0, i - 3)\n",
        "            window_end = min(len(tokens), i + 4)\n",
        "            window = tokens[window_start:window_end]\n",
        "\n",
        "            is_negated = False\n",
        "            local_score = 0\n",
        "\n",
        "            for w in window:\n",
        "                if w in negations:\n",
        "                    is_negated = True\n",
        "\n",
        "                word_score = get_adjective_sentiment(w)\n",
        "                if word_score != 0:\n",
        "                    local_score += word_score\n",
        "\n",
        "            if is_negated:\n",
        "                local_score = local_score * -1\n",
        "\n",
        "            if local_score != 0:\n",
        "                score += local_score\n",
        "                count += 1\n",
        "\n",
        "    if count > 0:\n",
        "        return score / count\n",
        "    return 0.0\n",
        "\n",
        "df = pd.read_csv('headphone.csv')\n",
        "df['clean_text'] = df['ReviewBody'].astype(str).str.lower()\n",
        "df['clean_text'] = df['clean_text'].apply(lambda x: re.sub(r'[^a-z\\s]', ' ', x))\n",
        "\n",
        "all_text = \" \".join(df['clean_text'].dropna().tolist())\n",
        "keywords = get_rake_keywords(all_text)\n",
        "top_aspects = [k[0] for k in keywords[:5]]\n",
        "\n",
        "print(\"Top Aspects Found:\", top_aspects)\n",
        "\n",
        "results = {}\n",
        "for aspect in top_aspects:\n",
        "    sentiments = []\n",
        "    for text in df['clean_text']:\n",
        "        s = analyze_sentiment(text, aspect)\n",
        "        if s != 0:\n",
        "            sentiments.append(s)\n",
        "\n",
        "    if len(sentiments) > 0:\n",
        "        avg_score = sum(sentiments) / len(sentiments)\n",
        "        results[aspect] = {'average_sentiment': avg_score, 'count': len(sentiments)}\n",
        "\n",
        "print(\"\\nFinal Analysis Results:\")\n",
        "print(pd.DataFrame(results).T)"
      ]
    }
  ]
}